<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Least Squares and QR Decomposition</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js">
  </script>
  <link rel="stylesheet" href="main.css">
</head>
<body>
   <h1>Least Squares and QR Decomposition - In Progress of Writing</h1>
   <h2>What is the linear least squares problem?</h2>
   <p>
   The linear least squares problem is to find a vector \(x\) of size \(n\) that minimizes \(||Ax-b||_2^2\), where \(b\) is a given vector of size \(m\) and \(A\) is a given \(m \times n\) matrix of full rank with \(m > n\).
   </p>
   <p>
   Some notes:
   <ul>
      <li>\(||v||_2^2 = v_1^2 + v_2^2 + \ldots + v_n^2\) for a vector \(v\), or for the complex case \(||v||_2^2 = v_1 \bar{v}_1 + v_2 \bar{v}_2 + \ldots + v_n \bar{v}_n\).
      <li>Entries of A and b can be real or complex.</li>
      <li>\(m > n\) means A is a tall skinny matrix, x is a short vector, and b is a tall vector.</li>
      <li>Hence, there are too few unknowns in x to solve Ax = b, so we have to settle for getting as close as possible.  Hence the minimization problem.</li>
      <li>A is of full rank means all the columns of A are linearly independent.  The purpose of this is to have only one solution to the problem.</li>
   </ul>
   <p>
   <h3><em>Example</em></h3>
   <p>
   An example of a linear least squares problem is a polynomial fit (regression) problem.
   Suppose you have 100 (x,y) coordinates that are suppose to fit closely to a quadratic.
   Then you want to find a quadratic \(y = a_0 + a_1 x + a_2 x^2\) that closely fits the coordinates.
   So you are trying to find coefficients \(a_0, a_1, a_2\) such that 
   \begin{align}
   y_1 &\approx a_0 + a_1 x_1 + a_2 x_1^2 \\
   y_2 &\approx a_0 + a_1 x_2 + a_2 x_2^2 \\
   \vdots \\
   y_{100} &\approx a_0 + a_1 x_{100} + a_2 x_{100}^2.
   \end{align}
   The linear least squares problem is to find the coefficients \(a_0, a_1, a_2\) that minimize 
   \begin{equation}
   (a_0 + a_1 x_1 + a_2 x_1^2 - y_1)^2 + \ldots + (a_0 + a_1 x_{100} + a_2 x_{100}^2 - y_{100})^2.
   \end{equation}
   If you form the matrix and vectors
   \begin{equation}
   A = \begin{pmatrix}
   1 & x_1 & x_1^2 \\
   1 & x_2 & x_2^2 \\
   \vdots & \vdots & \vdots \\
   1 & x_{100} & x_{100}^2
   \end{pmatrix}, \qquad
   x = \begin{pmatrix}
   a_0 \\ a_1 \\ a_2
   \end{pmatrix}, \qquad
   b = \begin{pmatrix}
   y_1 \\ y_2 \\ \vdots \\ y_{100}
   \end{pmatrix}
   \end{equation}
   you get the equivalent problem
   \begin{equation}
   \min_{x} ||Ax - b||_2^2.
   \end{equation}
   </p>

   <h2>The normal equations</h2>
   <p>
   One multivariable calculus technique to solve the minimization is to take the partial derivative with respect to \(x_1, x_2, \ldots, x_n\), set all the equations to zero, and solve for \(x_1, x_2, \ldots, x_n\).
   Writing the result in matrix form leads to the what is called the normal equations
   \begin{equation}
   A^T A x = A^T b.
   \end{equation}
   The nice thing about this system is that it's a small \(n \times n\) matrix system that can be solved.
   The problem with this formulation is that it squares the condition number of the problem.
   </p>
   <p>
   Briefly, the condition number is the largest eigenvalue in absolute value divided by the smallest eigenvalue in absolute value.
   It tells you how numerically solvable the problem is.
   A big condition number means the problem is difficult to solve numerically.
   Squaring a condition number can make a problem impossible to solve in some cases.
   </p>
   <p>
   One way to prove the condition number is squared is to take the singular value decomposition: \(A = U \Sigma V^T\).
   Then 
   \begin{equation}
   A^T A = V^T \Sigma^T \Sigma V,
   \end{equation}
   which shows the singular values have been squared and hence the condition number is squared.
   </p>



   <h2>The QR decomposition</h2>
   <p>
   The QR decomposition of a matrix \(A \in \mathbb{R}^{m \times n}\) is \(A = QR\) where \(Q \in \mathbb{R}^{m \times m}\) is an orthogonal matrix and \(R \in \mathbb{R}^{m \times n}\) is an upper triangular matrix.
   If A contains imaginary numbers then Q is unitary.
   For the case we care about, \(m > n\), R has the form
   \begin{equation}
   \begin{pmatrix}
   R_1 \\
   R_2
   \end{pmatrix} = 
   \begin{pmatrix}
   R_1 \\
   0
   \end{pmatrix}
   \end{equation}
   where R_1 is n by n and R_2 is m-n by n zero matrix.
   </p>
   <p>Some notes:
   <ul>
      <li>Q orthogonal means \(Q^T Q = Q Q^T = I\).</li>
      <li>Q unitary means \(Q^* Q = Q Q^* = I, \quad Q^* = \bar{Q}^T\).</li>
   </ul>
   </p>
   <h3>Solving the least squares problem with QR decomposition</h3>
   <p>
   The nice thing about an orthogonal matrix is it can move in and out of the 2 norm.
   Specifically, for any vector v,
   \begin{equation}
   ||Qv||_2 = ||Q|| \; ||v||_2 = ||v||_2.
   \end{equation}
   Proof:
   \begin{equation}
   ||Qv||_2 \leq ||Q|| \; ||v||_2 = ||v||_2.
   \end{equation}
   \begin{equation}
   ||v||_2 = ||Q^T Q v||_2 \leq ||Q^T|| \; ||Qv||_2 = ||Qv||_2.
   \end{equation}
   QED

   <p>
   Now for the method of solving the least squares problem.
   \begin{align}||Ax - b||_2 &= ||QRx - b||_2 \\
   &= ||Q^T Q Rx - Q^Tb||_2 \\
   &= ||Rx - Q^Tb||_2 \end{align}
   To minimize the last expression, write \(\tilde{b} = Q^T b\) and minimize
   \begin{equation}
   ||Rx - \tilde{b}||_2^2.
   \end{equation}
   Using the splitting for R given earlier, we have
   \begin{equation}
   \begin{pmatrix}
   R_1 \\ 0
   \end{pmatrix}
   x - 
   \begin{pmatrix}
   \tilde{b}_1 \\ \tilde{b}_2
   \end{pmatrix}
   \end{equation}
   which means we want to minimize
   \begin{equation}
   ||R_1 x - \tilde{b}_1||_2^2 + ||\tilde{b}_2||_2^2.
   \end{equation}
   We can make the first norm zero by solving the square triangular system.
   The second norm cannot be changed and is the error.
   </p>
   <p>
   One implementation detail about this is that for a tall skinny matrix, one can perform a <em>skinny</em> QR decomposition.
   This is given by A = QR where Q is m by n (tall and skinny) and R is n by n (a small square matrix.
   </p>
</body>
</html>
